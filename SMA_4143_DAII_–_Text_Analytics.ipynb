{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdXrCP40HBwH"
   },
   "source": [
    " # GROUP FOUR\n",
    "1. S084-01-2341/2021 PETER KARURI NDUNGU\n",
    "2. S084-01-2342/2021 MAURINE CHEMUTAI\n",
    "3. S084-01-2299/2021 SAMUEL MUTAHI\n",
    "4. S084-01-2301/2021 WINROSE WANGUI\n",
    "5. S084-01-2742/2021 AMOS KIPNGENO\n",
    "6. S084-01-2318/2021 ERICK MUEMA\n",
    "\n",
    "# SMA 4143 DAII â€“ Text Analytics\n",
    "1. **Define Text analytics and discuss its importance in data science.**   \n",
    " **Text Analytics** involves extracting meaningful information from text data using various computational techniques.  \n",
    " **Importance in Data Science:**  \n",
    " *  *Insights Extraction:*  \n",
    "  Helps in understanding trends, patterns, and sentiments from large text datasets.\n",
    "  *   *Decision Making:*  \n",
    " Facilitates better decision-making by providing a deeper understanding of textual data, such as trends, themes, and emerging issues.\n",
    "  *   *Automation:*  \n",
    "   Enables automation of tasks like categorizing documents, detecting spam, and summarizing content, increasing efficiency and productivity.\n",
    "\n",
    "2.  **Discuss the importance of text data pre-processing.**  \n",
    "* ***Noise Reduction:*** Removes irrelevant information (e.g., punctuation, stop words), making the data cleaner.\n",
    "****Normalization:*** Standardizes text data to ensure consistency, which is important for accurate analysis (e.g., converting all text to lowercase).\n",
    "****Improves Model Performance:*** Properly pre-processed text data enhances the performance of machine learning models by ensuring that the data is in a suitable format.\n",
    "****Feature Extraction:*** Facilitates the extraction of meaningful features from text, which can be used for various analytical purposes.\n",
    "\n",
    "**3. Explain the following techniques as used in text analytics and discuss their significance:**\n",
    "     \n",
    "***(a) Text Normalization:***\n",
    "\n",
    "* It is the process of converting text into standard format (e.g., lowercasing).\n",
    "\n",
    "*Significance:*   \n",
    "* Ensures uniformity in text data by handling variations such as case sensitivity, punctuation, and contractions (e.g., \"don't\" to \"do not\").  \n",
    "\n",
    "***(b) Tokenization:***  \n",
    "* It is the process of Splitting text into individual words or tokens.  \n",
    "\n",
    "*Significance:*  \n",
    "* Fundamental for many text processing tasks, enabling the analysis of text at the word level.\n",
    "\n",
    "***(c) Stop-word Removal:***\n",
    "* Eliminates common words that carry little meaning (e.g., \"and\", \"the\").\n",
    "\n",
    "*Significance:*  \n",
    " * Reduces dimensionality and focuses on more meaningful words.  \n",
    "\n",
    "***(d) Stemming:***  \n",
    "* Reduces words to their root form (e.g., \"running\" to \"run\").  \n",
    "\n",
    "*Significance:*   \n",
    "* Helps in standardizing words with similar meanings, improving the efficiency of text analysis.  \n",
    "\n",
    "***(e) Lemmatization:***  \n",
    "* Converts words to their dictionary form (e.g., \"better\" to \"good\").\n",
    "\n",
    "*Significance:*   \n",
    "* Provides more accurate word representations compared to stemming, aiding in better text understanding.  \n",
    "\n",
    "***(f) Text Encoding:***  \n",
    " * Converts text into numerical representations (e.g., one-hot encoding, TF-IDF).\n",
    "\n",
    "*Significance:*  \n",
    " * Necessary for machine learning models to process text data.\n",
    "\n",
    "***(g) Vectorization and Embeddings:***  \n",
    "* Transforms text into vector representations (e.g., Word2Vec, BERT embeddings).  \n",
    "\n",
    "*Significance:*  \n",
    " * Captures semantic meaning and context of words, improving model performance.\n",
    "\n",
    "***(h) Padding/Truncation:***  \n",
    "* Adjusts text sequences to a uniform length by adding padding or truncating.\n",
    "\n",
    "*Significance:*   \n",
    "* Ensures consistency in input lengths for models, particularly in deep learning.\n",
    "\n",
    "\n",
    " **4. What is sentiment analysis? Describe a scenario where sentiment  analysis can provide valuable insights.**  \n",
    " * It is the process of determining the emotional tone behind a series of words, used to understand the attitudes, opinions, and emotions expressed within a text.\n",
    "\n",
    " * *Scenario:*  \n",
    " A company launching a new product can use sentiment analysis on social media posts and customer reviews to gauge public reaction. Positive sentiments can affirm marketing strategies, while negative sentiments can highlight areas for improvement, leading to better customer satisfaction and product refinement.\n",
    "\n",
    "**5. Explain how sentiment analysis can be performed using machine learning techniques.**  \n",
    "* Sentiment analysis using machine learning techniques involves several steps, from data collection and pre-processing to model training, evaluation, and prediction.   \n",
    "\n",
    "***Steps to Perform Sentiment Analysis Using Machine Learning***  \n",
    "* *Data Collection:*  \n",
    " * Gather a dataset containing text data with sentiment labels.\n",
    "\n",
    "* *Data Pre-processing:*\n",
    "  * *Text Cleaning:*  \n",
    "Remove noise such as  special characters and URLs.  \n",
    "  * *Text Normalization:*  \n",
    " Convert all text to lowercase to ensure uniformity.\n",
    "  * *Tokenization:*  \n",
    " Split the text into individual words or tokens.\n",
    " * *Stop-word Removal:*\n",
    " Remove common words (e.g., \"and\", \"the\") that do not carry significant meaning.  \n",
    "\n",
    " * *Stemming/Lemmatization:*   \n",
    "Reduce words to their root form to standardize them.\n",
    "Example: Convert \"The movie was fantastic!\" to [\"movie\", \"fantastic\"].  \n",
    "* *Feature Extraction:*  \n",
    "Convert text data into numerical representations that can be fed into a machine learning model.\n",
    "     * *Bag of Words (BoW):*   \n",
    "     Represent text as a vector of word counts.\n",
    "\n",
    "     * *TF-IDF (Term Frequency-Inverse Document Frequency):*  \n",
    "      Weigh the frequency of words by their importance.\n",
    "     * *Word Embeddings:*  \n",
    "      Use pre-trained embeddings like Word2Vec, GloVe, or contextual embeddings like BERT to capture semantic meaning.\n",
    "Example: \"The movie was fantastic\" -> [0.1, 0.2, ..., 0.5] (vector representation).\n",
    "*  *Model Selection:*       \n",
    "  * Choose an appropriate machine learning algorithm for example **Naive Bayes** *which  assumes independence between features and works well for text classification*, **Support Vector Machines (SVM):** *which is effective for high-dimensional spaces and text classification*, **Logistic Regression:** *which is a simple and efficient baseline for binary classification tasks.*  \n",
    "\n",
    "*  *Training the Model:*  \n",
    "  * Splits the dataset into training and validation sets.  \n",
    "  * Train the selected model on the training data.\n",
    "\n",
    "*  *Model Evaluation:*  \n",
    " * Evaluate the model's performance using metrics like accuracy, precision, recall and F1-score.\n",
    " * Use a validation set to tune hyperparameters and avoid overfitting.\n",
    "\n",
    "*  *Prediction*  \n",
    " * Apply the trained model to new, unseen text data to predict sentiment.\n",
    "Example: Predict the sentiment of new movie reviews using the trained model.\n",
    "* Post-processing and Visualization:*  \n",
    " * Analyze the results and visualize the sentiment distribution, trends, and patterns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9wpEK2MiZox"
   },
   "source": [
    "6. **Write a python code to preprocess the Text data provided. Submit a comprehensive Jupiter\n",
    "notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nm-UlhiEGY3P",
    "outputId": "63ee0495-aa92-48c3-b532-c18bf7743043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Normalized Text: \n",
      "once upon a time, in a small village, there lived a quick brown fox named felix. he was\n",
      "famous for his speed and agility. ðŸ¦Š felix could run at 25 miles per hour (mph) and jump over\n",
      "obstacles twice his height! ðŸŒŸ\n",
      "\n",
      "\n",
      " Tokens:\n",
      " ['once', 'upon', 'a', 'time', ',', 'in', 'a', 'small', 'village', ',', 'there', 'lived', 'a', 'quick', 'brown', 'fox', 'named', 'felix', '.', 'he', 'was', 'famous', 'for', 'his', 'speed', 'and', 'agility', '.', 'ðŸ¦Š', 'felix', 'could', 'run', 'at', '25', 'miles', 'per', 'hour', '(', 'mph', ')', 'and', 'jump', 'over', 'obstacles', 'twice', 'his', 'height', '!', 'ðŸŒŸ']\n",
      "\n",
      " Filtered Tokens:\n",
      " ['upon', 'time', 'small', 'village', 'lived', 'quick', 'brown', 'fox', 'named', 'felix', 'famous', 'speed', 'agility', 'felix', 'could', 'run', '25', 'miles', 'per', 'hour', 'mph', 'jump', 'obstacles', 'twice', 'height']\n",
      "\n",
      "Stemmed Tokens:\n",
      " ['upon', 'time', 'small', 'villag', 'live', 'quick', 'brown', 'fox', 'name', 'felix', 'famou', 'speed', 'agil', 'felix', 'could', 'run', '25', 'mile', 'per', 'hour', 'mph', 'jump', 'obstacl', 'twice', 'height']\n",
      "\n",
      "Lemmatized Tokens:\n",
      " ['upon', 'time', 'small', 'village', 'lived', 'quick', 'brown', 'fox', 'named', 'felix', 'famous', 'speed', 'agility', 'felix', 'could', 'run', '25', 'mile', 'per', 'hour', 'mph', 'jump', 'obstacle', 'twice', 'height']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Sample text data from the provided document\n",
    "text_data = \"\"\"\n",
    "Once upon a time, in a small village, there lived a QUICK brown fox named Felix. He was\n",
    "famous for his speed and agility. ðŸ¦Š Felix could run at 25 miles per hour (mph) and jump over\n",
    "obstacles twice his height! ðŸŒŸ\n",
    "\"\"\"\n",
    "\n",
    "# 1. Text Normalization\n",
    "text_data = text_data.lower()\n",
    "\n",
    "# 2. Tokenization\n",
    "tokens = word_tokenize(text_data)\n",
    "\n",
    "# 3. Stop-word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# 4. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "# 5. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "# Print the results\n",
    "print(\" Normalized Text:\", text_data)\n",
    "print(\"\\n Tokens:\\n\", tokens)\n",
    "print(\"\\n Filtered Tokens:\\n\", filtered_tokens)\n",
    "\n",
    "print(\"\\nStemmed Tokens:\\n\", stemmed_tokens)\n",
    "print(\"\\nLemmatized Tokens:\\n\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biIJO3RbG6CR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
